{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP). It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning.\n",
    "\n",
    "The NLTK corporation and modules must be installed using the standard NLTK downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download the specific packages or nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The news dataset comprises various authors' original and fictitious article titles and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "news_data = pd.read_csv(\"Data/news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of News data: \", news_data.shape)\n",
    "print(\"News data columns: \", news_data.columns)\n",
    "print(\"News data info:\")\n",
    "news_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Familiarizing with the dataset by viewing first 5 rows of every column. \n",
    "news_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Word statistics: min.mean, max and interquartile range\n",
    "\n",
    "txt_length = news_data['text'].str.split().str.len()\n",
    "txt_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Title statistics \n",
    "\n",
    "title_length = news_data.title.str.split().str.len()\n",
    "title_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistics for the training and testing sets are as follows:\n",
    "- The text attribute has a higher word count with an average of 776 words and 75% having more than 1000 words.\n",
    " -The title attribute is a short statement with an average of 10 words, and 75% of them are around 13 words.\n",
    " - The experiment would be with both text and title together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(dataset):\n",
    "    # Removed id column\n",
    "    data = dataset.drop(columns=['Unnamed: 0'])\n",
    "    \n",
    "    # Define nested/inner function to impute null values with None\n",
    "    def replace_null(data):\n",
    "        for col in data:\n",
    "            data.loc[data[col].isnull(), col] = \"None\"\n",
    "        return data\n",
    "    \n",
    "    # Apply method to data\n",
    "    df = replace_null(data)\n",
    "    \n",
    "    # Define nested/inner function to clean text and title columns\n",
    "    def clean_text(text):\n",
    "        # Remove urls, then remove everything else except words (w) and punctuation (s)\n",
    "        text = re.sub(r'http[\\w:/\\.]+', ' ', str(text))\n",
    "        text = re.sub(r'[^\\.\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "        text = re.sub(r'\\s\\s+', ' ', text)\n",
    "        # Turn text to lowercase & use strip to remove the whitespace from the beginning & end of the string\n",
    "        text = text.lower().strip()\n",
    "        # Split string by word into array of words\n",
    "        wordlist = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "        # Remove words not in stopwords_dict: words with no significance such as (but, and, or), then rejoin string\n",
    "        text = ' '.join([wnl.lemmatize(word) for word in wordlist if word not in stopwords_dict])\n",
    "        return text\n",
    "    \n",
    "    # Apply method to text & title columns\n",
    "    df['text'] = df['text'].apply(clean_text)\n",
    "    df['title'] = df['title'].apply(clean_text)\n",
    "    \n",
    "    # Define another inner function to change label column from text to 0 if REAL and 1 if FAKE\n",
    "    def category_sort(label):\n",
    "        if label == 'FAKE':\n",
    "            return 1\n",
    "        elif label == 'REAL':\n",
    "            return 0\n",
    "        return label\n",
    "    \n",
    "    # Apply method to the label column    \n",
    "    df['label'] = df['label'].apply(category_sort).astype(int)\n",
    "    \n",
    "    # Set precision point to remove float\n",
    "    pd.set_option('display.precision', 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle(news_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize word cloud\n",
    "wordcloud = WordCloud( background_color='black', width=800, height=600)\n",
    "# Passing in the text\n",
    "text_cloud = wordcloud.generate(' '.join(df['text']))\n",
    "# Plotting the result:\n",
    "plt.figure(figsize=(20,30))\n",
    "plt.imshow(text_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#Save plot\n",
    "plt.savefig('saved_plots/text-cloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For reliable or REAL news only, filter/mask text where label == 0\n",
    "real_news = ' '.join(df[df['label']==0]['text']) \n",
    "text_cloud_real = wordcloud.generate(real_news)\n",
    "#Plotting the result:\n",
    "plt.figure(figsize=(20,30))\n",
    "plt.imshow(text_cloud_real)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#Save plot\n",
    "plt.savefig('saved_plots/text-cloud-real.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For FAKE news only, filter/mask text where label == 1\n",
    "fake_news = ' '.join(df[df['label']==1]['text']) \n",
    "text_cloud_fake = wordcloud.generate(fake_news)\n",
    "#Plotting the result:\n",
    "plt.figure(figsize=(20,30))\n",
    "plt.imshow(text_cloud_fake)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#Save plot\n",
    "plt.savefig('saved_plots/text-cloud-fake.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot count of REAL AND FAKE news\n",
    "sns.countplot(x=\"label\", data=df);\n",
    "plt.xlabel('Real & Fake News')\n",
    "plt.ylabel('Count')\n",
    "plt.title(\"Distribution of labels\")\n",
    "print(df.label.value_counts());\n",
    "\n",
    "#Save Figure\n",
    "plt.savefig('saved_plots/real_&_fake-count_barplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot an n-gram\n",
    "def plot_top_ngrams(corpus, title, ylabel, xlabel=\"Number of Occurences\", n=2):\n",
    "  \"\"\"Utility function to plot top n-grams\"\"\"\n",
    "  true_b = (pd.Series(nltk.ngrams(corpus.split(), n)).value_counts())[:20]\n",
    "  true_b.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "  plt.title(title)\n",
    "  plt.ylabel(ylabel)\n",
    "  plt.xlabel(xlabel)\n",
    "  plt.savefig(f'saved_plots/{title}.png')\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the most common bigram on the reliable news:\n",
    "plot_top_ngrams(real_news, 'Top 20 Frequently Occuring Real news Bigrams', \"Bigram\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the most common bigram on the fake news:\n",
    "plot_top_ngrams(fake_news, 'Top 20 Frequently Occuring Fake news Bigrams', \"Bigram\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the most common three word combination on the reliable news:\n",
    "plot_top_ngrams(real_news, 'Top 20 Frequently Occuring Real news Trigrams', \"Trigrams\", n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the most common trigram on the fake news:\n",
    "plot_top_ngrams(fake_news, 'Top 20 Frequently Occuring Fake news Trigrams', \"Trigrams\", n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the news analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
